# -*- coding: utf-8 -*-
"""leukemia.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a6NlOkNasJiYvEXAZjFqWAZPKPxQ0cij

# Download dataset

# Display Images
"""



import numpy as np
import cv2
import numpy as np
from sklearn.cluster import KMeans
from skimage.segmentation import watershed

# Define functions for image preprocessing
def gaussian_filter(image):
    return cv2.GaussianBlur(image, (5, 5), 0)

def wiener_filter(image):
    return cv2.fastNlMeansDenoisingColored(image, None, 10, 10, 7, 21)

def adaptive_median_filter(image):
    return cv2.medianBlur(image, 5)

# Load and preprocess the dataset
def preprocess_image(image):
    # image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    image_gaussian = gaussian_filter(image)
    image_wiener = wiener_filter(image_gaussian)
    image_adaptive = adaptive_median_filter(image_wiener)
    return image, image_gaussian, image_wiener, image_adaptive

# Automatic segmentation using KMeans clustering
def kmeans_segmentation(image):
    flattened_image = image.reshape((-1, 3))
    kmeans = KMeans(n_clusters=10)
    kmeans.fit(flattened_image)
    segmented_image = kmeans.labels_.reshape(image.shape[:2])
    return segmented_image

def apply_kmeans_segmentation(image, segmented_image):
    num_clusters = np.max(segmented_image) + 1
    centroids = []
    for i in range(num_clusters):
        cluster_pixels = image[segmented_image == i]
        centroids.append(np.mean(cluster_pixels, axis=0))
    centroids = np.array(centroids, dtype=np.uint8)
    segmented_image_colored = centroids[segmented_image]
    # segmented_image_colored = cv2.cvtColor(segmented_image_colored, cv2.COLOR_RGB2BGR)
    return segmented_image_colored

#Saving stacked images for each step

import os
import random
import cv2
from google.colab.patches import cv2_imshow
# Function to get a random image path from each folder
def get_random_image_paths(root_dir):
    random_image_paths = []
    for folder in os.listdir(root_dir):
        folder_path = os.path.join(root_dir, folder)
        if os.path.isdir(folder_path):
            image_files = [f for f in os.listdir(folder_path) if f.endswith('.jpg') or f.endswith('.png')]
            if image_files:
                random_image = random.choice(image_files)
                random_image_path = os.path.join(folder_path, random_image)
                random_image_paths.append((folder, random_image_path))
    return random_image_paths

# Function to process and display stacked images for random images from each folder
def process_and_display_random_images(root_dir):
    random_image_paths = get_random_image_paths(root_dir)
    for folder, image_path in random_image_paths:
        image = cv2.imread(image_path)
        original_image, image_gaussian, image_wiener, image_adaptive = preprocess_image(image)
        segmented_image_kmeans = kmeans_segmentation(image_adaptive)
        segmented_image_kmeans_colored = apply_kmeans_segmentation(image_adaptive, segmented_image_kmeans)
        save_and_display_images(original_image, image_gaussian, image_wiener, image_adaptive, segmented_image_kmeans_colored, folder)

# Modified save_and_display_images function to display folder names
def save_and_display_images(original_image, image_gaussian, image_wiener, image_adaptive, segmented_image_kmeans_colored, folder_name):
    # Display images side by side
    original_text = "Original Image"
    gaussian_text = "Gaussian Filter"
    wiener_text = "Wiener Filter"
    adaptive_text = "Adaptive Median Filter"
    kmeans_text = "KMeans Segmentation"
    folder_text = f"Class: {folder_name}"

    font = cv2.FONT_HERSHEY_SIMPLEX
    font_scale = 0.5
    font_thickness = 1
    text_color = (255, 255, 255)
    text_org = (10, 20)

    original_image = cv2.putText(original_image, original_text, text_org, font, font_scale, text_color, font_thickness, cv2.LINE_AA)
    gaussian_image = cv2.putText(image_gaussian, gaussian_text, text_org, font, font_scale, text_color, font_thickness, cv2.LINE_AA)
    wiener_image = cv2.putText(image_wiener, wiener_text, text_org, font, font_scale, text_color, font_thickness, cv2.LINE_AA)
    adaptive_image = cv2.putText(image_adaptive, adaptive_text, text_org, font, font_scale, text_color, font_thickness, cv2.LINE_AA)
    kmeans_image = cv2.putText(segmented_image_kmeans_colored, kmeans_text, text_org, font, font_scale, text_color, font_thickness, cv2.LINE_AA)
    folder_image = cv2.putText(segmented_image_kmeans_colored, folder_text, (10, 40), font, font_scale, text_color, font_thickness, cv2.LINE_AA)

    stacked_images = np.hstack((original_image, gaussian_image, wiener_image, adaptive_image, kmeans_image))
    print(f"##{folder_name}")
    cv2_imshow( stacked_images)
    #cv2.waitKey(0)
    #cv2.destroyAllWindows()

# Call the function with the root directory
root_dir = "/content/luke/archive (1)/test"
process_and_display_random_images(root_dir)

"""# resize"""

# Resizing initial dataset to 224
# Define constants and paths
DATA_DIR = "archive_19"  # Replace with your actual path
OUTPUT_DIR = "working"  # Replace with your desired output path
TARGET_SIZE = (224, 224)  # Replace with your chosen target size

# Function to resize images using TensorFlow GPU support
def resize_image(input_filepath, output_filepath):
    img = tf.io.read_file(input_filepath)
    img = tf.image.decode_image(img, channels=3)
    img = tf.image.resize(img, TARGET_SIZE)
    img = tf.cast(img, tf.uint8)
    img = tf.image.encode_jpeg(img)

    with tf.io.gfile.GFile(output_filepath, 'wb') as file:
        file.write(img.numpy())

# Traverse through all subdirectories
for root, dirs, files in os.walk(DATA_DIR):
    for filename in tqdm(files):
        # Construct input and output filepaths
        input_filepath = os.path.join(root, filename)
        relative_path = os.path.relpath(input_filepath, DATA_DIR)
        output_filepath = os.path.join(OUTPUT_DIR, relative_path)

        # Create output directory if it doesn't exist
        os.makedirs(os.path.dirname(output_filepath), exist_ok=True)

        # Resize and save the image using TensorFlow GPU support
        resize_image(input_filepath, output_filepath)

"""# preprocess dataset"""

#functions to preprocess and segment resized images

import numpy as np
import cv2
import numpy as np
from sklearn.cluster import KMeans
from skimage.segmentation import watershed


# Define functions for image preprocessing
def gaussian_filter(image):
    return cv2.GaussianBlur(image, (5, 5), 0)

def wiener_filter(image):
    return cv2.fastNlMeansDenoisingColored(image, None, 10, 10, 7, 21)

def adaptive_median_filter(image):
    return cv2.medianBlur(image, 5)
# Load and preprocess the dataset
def preprocess_image(image):
    # image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    image = gaussian_filter(image)
    image = wiener_filter(image)
    image = adaptive_median_filter(image)
    return image
# Automatic segmentation using KMeans clustering
def kmeans_segmentation(image):
    flattened_image = image.reshape((-1, 3))
    kmeans = KMeans(n_clusters=10)
    kmeans.fit(flattened_image)
    segmented_image = kmeans.labels_.reshape(image.shape[:2])
    return segmented_image

def apply_kmeans_segmentation(image, segmented_image):
    num_clusters = np.max(segmented_image) + 1
    centroids = []
    for i in range(num_clusters):
        cluster_pixels = image[segmented_image == i]
        centroids.append(np.mean(cluster_pixels, axis=0))
    centroids = np.array(centroids, dtype=np.uint8)
    segmented_image_colored = centroids[segmented_image]
    # segmented_image_colored=cv2.cvtColor(segmented_image_colored, cv2.COLOR_RGB2BGR)
    return segmented_image_colored



def seggment(image):
    preprocessed_image = preprocess_image(image)
    segmented_image_kmeans = kmeans_segmentation(preprocessed_image)
    segmented_image_kmeans_colored = apply_kmeans_segmentation(preprocessed_image, segmented_image_kmeans)
    return segmented_image_kmeans_colored

# preprocess and segment dataset

import os
import cv2
import numpy as np
from scipy.signal import wiener
from scipy.ndimage import median_filter
from tqdm import tqdm

# Path to the main directory containing train and test directories
main_dir = "/content/luke/archive (1)"

output_dir = "/content/luke/process"


# Create output directory if it does not exist
if not os.path.exists(output_dir):
    os.makedirs(output_dir)



# Function to preprocess images in a directory
def preprocess_images_in_dir(input_dir, output_dir):
    for category in os.listdir(input_dir):
        input_category_dir = os.path.join(input_dir, category)
        output_category_dir = os.path.join(output_dir, category)
        if not os.path.exists(output_category_dir):
            os.makedirs(output_category_dir)
        for filename in tqdm(os.listdir(input_category_dir)):
            if filename.endswith(".jpg") or filename.endswith(".png"): # Add more image extensions if necessary
                print(os.path.join(input_category_dir, filename))
                img_path = os.path.join(input_category_dir, filename)
                img = cv2.imread(img_path)
                if img is not None:
                    segmented_image_kmeans_colored=seggment(img)
                    cv2.imwrite(os.path.join(output_category_dir, "segmented_image_kmeans_" + filename), segmented_image_kmeans_colored)

# Preprocess images in train and test directories
preprocess_images_in_dir(os.path.join(main_dir, "train"), os.path.join(output_dir, "train"))
preprocess_images_in_dir(os.path.join(main_dir, "test"), os.path.join(output_dir, "test"))



"""# old model code"""

# ## import os
# import numpy as np
# from tensorflow.keras.preprocessing.image import ImageDataGenerator
# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

# # Define directories for train and test data
# train_dir = 'process/train'
# test_dir = 'process/test'

# # Define image dimensions and batch size
# img_width, img_height = 128, 128
# batch_size = 512

# # Data preprocessing and augmentation
# train_datagen = ImageDataGenerator(
#     rescale=1./255,
#     shear_range=0.2,
#     zoom_range=0.2,
#     horizontal_flip=True)

# test_datagen = ImageDataGenerator(rescale=1./255)

# train_generator = train_datagen.flow_from_directory(
#     train_dir,
#     target_size=(img_width, img_height),
#     batch_size=batch_size,
#     class_mode='categorical')

# test_generator = test_datagen.flow_from_directory(
#     test_dir,
#     target_size=(img_width, img_height),
#     batch_size=batch_size,
#     class_mode='categorical')


# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
# from tensorflow.keras.optimizers import Adam
# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
# from tensorflow.keras import regularizers

# # Define the model
# model = Sequential([
#     Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)),
#     MaxPooling2D((2, 2)),
#     Conv2D(64, (3, 3), activation='relu'),
#     MaxPooling2D((2, 2)),
#     Conv2D(128, (3, 3), activation='relu'),
#     MaxPooling2D((2, 2)),
#     # Conv2D(256, (3, 3), activation='relu'),
#     # MaxPooling2D((2, 2)),
#     Flatten(),
#     Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)),
#     Dropout(0.5),
#     Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)),
#     Dropout(0.5),
#     Dense(5, activation='softmax')  # Adjust the output size according to your number of classes
# ])

# # Compile the model
# model.compile(optimizer=Adam(lr=0.0001),
#               loss='categorical_crossentropy',
#               metrics=['accuracy'])

# # Define callbacks
# early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)
# # reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)

# # Train the model
# history = model.fit(
#     train_generator,
#     steps_per_epoch=train_generator.samples // batch_size,
#     epochs=50,  # Increasing the number of epochs for better training
#     validation_data=test_generator,
#     validation_steps=test_generator.samples // batch_size,
#     callbacks=[early_stopping])
# # , reduce_lr
# # Evaluate the model
# loss, accuracy = model.evaluate(test_generator)
# print(f'Test Accuracy: {accuracy}')

# # Plot training history
# import matplotlib.pyplot as plt

# plt.plot(history.history['accuracy'], label='train_accuracy')
# plt.plot(history.history['val_accuracy'], label='val_accuracy')
# plt.title('Training and Validation Accuracy')
# plt.xlabel('Epochs')
# plt.ylabel('Accuracy')
# plt.legend()
# plt.show()

# # Evaluate the model
# loss, accuracy = model.evaluate(test_generator)
# print(f'Test Accuracy: {accuracy}')
# # Evaluate the model
# loss, accuracy = model.evaluate(train_generator)
# print(f'Train Accuracy: {accuracy}')

# # Plot training and validation loss curves
# plt.figure(figsize=(12, 6))

# plt.subplot(1, 2, 1)
# plt.plot(history.history['loss'], label='train_loss')
# plt.plot(history.history['val_loss'], label='val_loss')
# plt.title('Training and Validation Loss')
# plt.xlabel('Epochs')
# plt.ylabel('Loss')
# plt.legend()

# plt.subplot(1, 2, 2)
# plt.plot(history.history['accuracy'], label='train_accuracy')
# plt.plot(history.history['val_accuracy'], label='val_accuracy')
# plt.title('Training and Validation Accuracy')
# plt.xlabel('Epochs')
# plt.ylabel('Accuracy')
# plt.legend()

# plt.show()

"""# Load dataset"""

# libraries

from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img
from tensorflow.keras.models import Sequential , Model
from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Conv2D, MaxPooling2D
import matplotlib.pyplot as plt
from glob import glob
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')
from sklearn.metrics import classification_report, confusion_matrix
import os
from tqdm import tqdm
import cv2
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TensorBoard, ModelCheckpoint

# name the labels
labels=['ALL','AML','CLL','CML','H']

# load the images

X_full = []
y_full = []
image_size = 128
for i in labels:
    folderPath = os.path.join('/content/luke/process/test',i)
    for j in tqdm(os.listdir(folderPath)):
        img = cv2.imread(os.path.join(folderPath,j))
        img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
        img = cv2.resize(img,(image_size, image_size))
        X_full.append(img)
        y_full.append(i)
for i in labels:
    folderPath = os.path.join('/content/luke/process/train',i)
    for j in tqdm(os.listdir(folderPath)):
        img = cv2.imread(os.path.join(folderPath,j))
        img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
        img = cv2.resize(img,(image_size, image_size))
        X_full.append(img)
        y_full.append(i)


X_full = np.array(X_full)
y_full = np.array(y_full)

#label encoding

y_full_new = []
for i in y_full:
    y_full_new.append(labels.index(i))
y_full = y_full_new
y_full = tf.keras.utils.to_categorical(y_full)

"""# split"""

#train test split

X_train,X_test,y_train,y_test = train_test_split(X_full,y_full, test_size=0.3,random_state=10)

"""# model"""

#transferlearn on pretrained model

from keras.applications import ResNet50V2, InceptionV3, MobileNetV2, EfficientNetV2B0,Xception,EfficientNetB7,ResNet50V2
from keras.layers import Dense, Dropout, GlobalAveragePooling2D, BatchNormalization,Flatten

from keras.optimizers import Adam
from keras.models import Sequential
from keras.layers import Dense, Dropout, GlobalAveragePooling2D

def train_models(X_train, X_test, y_train, y_test):
    # Define image size
    image_size = 128
    img_size = (image_size, image_size)

    # Create model architecture
    # models = [ResNet50V2, EfficientNetB7, MobileNetV2, Xception]

    models = [EfficientNetB7]

    history=[]
    mods=[]
    for model in models:
        # Load pre-trained model
        base_model = model(include_top=False, weights='imagenet', input_shape=(*img_size, 3))

        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2, min_delta=0.001, mode='auto', verbose=1)

        # Freeze pre-trained layers
        for layer in base_model.layers:
            layer.trainable = False

        # Add custom layers
        model = Sequential()
        model.add(base_model)
        model.add(GlobalAveragePooling2D())
        # model.add(Flatten())
        model.add(Dense(512, activation='relu'))
        model.add(BatchNormalization())
        model.add(Dropout(0.5))
        model.add(Dense(256, activation='relu'))
        model.add(BatchNormalization())
        model.add(Dropout(0.5))
        model.add(Dense(5, activation='softmax'))


        # Compile model with custom learning rate
        optimizer = Adam(learning_rate=0.0001)
        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

        # Fit model
        hist=model.fit(X_train,
                 y_train,
                 validation_split=0.2,
                 verbose=1,
                 batch_size=64, epochs=20,
                 callbacks=reduce_lr)
        history.append(hist)
        mods.append(model)
    return history, mods

history, mods=train_models(X_train, X_test, y_train, y_test)

"""# evaluate"""

#evaluation of model

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
model_names =['EfficientNetB7']
def plot_history(history, model_name):
    # Plot training & validation accuracy values
    plt.plot(history.history['accuracy'], label='Train')
    plt.plot(history.history['val_accuracy'], label='Validation')
    plt.title(f'Model accuracy - {model_name}')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend()
    plt.show()

    # Plot training & validation loss values
    plt.plot(history.history['loss'], label='Train')
    plt.plot(history.history['val_loss'], label='Validation')
    plt.title(f'Model loss - {model_name}')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend()
    plt.show()

def evaluate_model(model, X_train, y_train, X_test, y_test, model_name):
    # Evaluate the model
    results_train = model.evaluate(X_train, y_train)
    results_test = model.evaluate(X_test, y_test)
    train_accuracy = results_train[1] * 100  # Assuming accuracy is the second element in the results_train tuple/list
    test_accuracy = results_test[1] * 100  # Assuming accuracy is the second element in the results_test tuple/list

    # Print the evaluation results
    print(f"{model_name} - Train Accuracy: {train_accuracy:.2f}%")
    print(f"{model_name} - Test Accuracy: {test_accuracy:.2f}%")

    # Print classification report
    pred_train = np.argmax(model.predict(X_train), axis=1)
    pred_test = np.argmax(model.predict(X_test), axis=1)
    print(f"{model_name} - Classification Report (Train):\n", classification_report(np.argmax(y_train, axis=1), pred_train))
    print(f"{model_name} - Classification Report (Test):\n", classification_report(np.argmax(y_test, axis=1), pred_test))

    # Calculate confusion matrix
    cm_train = confusion_matrix(np.argmax(y_train, axis=1), pred_train)
    cm_test = confusion_matrix(np.argmax(y_test, axis=1), pred_test)

    # Plot confusion matrix
    plot_confusion_matrix(cm_train, model_name + " (Train)")
    plot_confusion_matrix(cm_test, model_name + " (Test)")

    # Plot ROC curve
    plot_roc_curve(model, X_train, y_train, X_test, y_test, model_name)

def plot_confusion_matrix(cm, title):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix - {title}')
    plt.xlabel('Predicted label')
    plt.ylabel('True label')
    plt.show()

def plot_roc_curve(model, X_train, y_train, X_test, y_test, model_name):
    # Calculate the predicted probabilities for each class
    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)

    # Calculate the micro-averaged false positive rate (FPR), true positive rate (TPR), and area under the curve (AUC)
    fpr_train, tpr_train, _ = roc_curve(y_train.ravel(), y_pred_train.ravel())
    fpr_test, tpr_test, _ = roc_curve(y_test.ravel(), y_pred_test.ravel())
    roc_auc_train = auc(fpr_train, tpr_train)
    roc_auc_test = auc(fpr_test, tpr_test)

    # Plot the ROC curve
    plt.figure(figsize=(8, 8))
    plt.plot(fpr_train, tpr_train, label=f'{model_name} (Train set) (AUC = {roc_auc_train:.2f})')
    plt.plot(fpr_test, tpr_test, label=f'{model_name} (Test set) (AUC = {roc_auc_test:.2f})')
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve - Model Comparison')
    plt.legend(loc='lower right')
    plt.show()

# Iterate over models
for i, (model, model_name) in enumerate(zip(mods, model_names)):
    plot_history(history[i], model_name)
    evaluate_model(model, X_train, y_train, X_test, y_test, model_name)

!pip install -q streamlit

!npm install localtunnel

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from PIL import Image
# import numpy as np
# from tensorflow.keras.preprocessing import image
# from tensorflow.keras.applications.efficientnet import preprocess_input
# from tensorflow.keras.optimizers import Adam
# import numpy as np
# from PIL import Image
# from tensorflow.keras.preprocessing import image
# from tensorflow.keras.applications.efficientnet import preprocess_input
# from keras.models import load_model
# st. set_page_config(page_title="Leukemia Detection App",layout="wide",page_icon="⚠️")
# st.markdown("""
#         <style>
#                 .reportview-container {
#                     margin-top: -2em;
#                 }
#                 #MainMenu {visibility: hidden;}
#                 .stDeployButton {display:none;}
#                 footer {visibility: hidden;}
#                 #stDecoration {display:none;}
#                 #MainMenu {visibility: hidden;}
#                 footer {visibility: hidden;}
#                .block-container {
#                     padding-top: 1rem;
#                     padding-bottom: 0rem;
#                     padding-left: 5rem;
#                     padding-right: 5rem;
#                 }
#         </style>
#         """, unsafe_allow_html=True)
# 
# 
# # Load a saved model
# model = load_model('/content/model_0.h5')
# #model = load_model('/content/model_0.h5', compile=False)
# #ensemble_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001), metrics=['accuracy'])
# 
# 
# def preprocess_image(image_path):
#     img = Image.open(image_path)
#     img = img.resize((128, 128))  # Resize the image
#     img_array = np.array(img)
#     img_array = np.expand_dims(img_array, axis=0)
#     img_array = preprocess_input(img_array)
#     return img_array
# 
# def predict_image_class(image_path):
#     processed_image = preprocess_image(image_path)
#     prediction = model.predict(processed_image)
#     predicted_class = np.argmax(prediction)
#     class_labels = ['ALL','AML','CLL','CML','H']   # Make sure this matches the order of your classes
#     predicted_label = class_labels[predicted_class]
#     return predicted_label, prediction
# # Streamlit UI
# st.title("Leukemia Detection App",anchor=None)
# 
# uploaded_file = st.file_uploader("Choose an image...", type=["jpg", "jpeg", "png"])
# 
# if uploaded_file is not None:
#     # Display uploaded image
#     image = Image.open(uploaded_file)
#     st.image(image, caption='Uploaded Image', width=224)
#     if st.button(label="Predict") and uploaded_file is not None:
#         # Make prediction on the uploaded image
#         with st.spinner('Predicting'):
#             predicted_label, prediction = predict_image_class(uploaded_file)
#         # Display prediction results
#         st.write("### Prediction")
#         st.write(f"Predicted Class: {predicted_label}")
#         st.write(f"Prediction Probabilities: {prediction}")

!streamlit run /content/app.py &>/content/logs.txt &

!npx localtunnel --port 8501 & curl https://ipv4.icanhazip.com